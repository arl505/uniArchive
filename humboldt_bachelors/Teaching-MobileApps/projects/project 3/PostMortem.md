## Name: Alec Levin   
# Post Mortem

Overall, I feel very happy with how this assignment went for me. In PA2, the image effects app, I overcame struggles that allowed me to have a much greater knowledge of Android C# programming and I felt confident going in to PA3. My deliverable for PA2 included the necessary Launch Camera Button, picture taking abilities (take picture function), and successfully started the Activity Result and loaded the image into a bitmap. With my understanding of these basic functions, I was already well on my way for PA3.    
      
Then, of course, I started to struggle. First, I had to understand how to load the most likely Label Annotation Description. This ended up being a simple single line of code. Struggling with this helped me understand the abilities of the Google Vision API and exactly how and what it sent back. Now I was at a point where the user could take a photo and I was able to load the photo on a new screen with a message asking if it was the most likely thing Google returned. This is where I incurred my biggest struggle during this assignment.      
      
I knew that from this point, I wanted to have multiple screens of different paths that a user could take through my application. For example, if they said no, I wanted the user to be taken to a screen where they could input what the photo was of. Then, if that thing was in the list of descriptions returned by Google, I wanted the user to be taken to a screen that reported the confidence level that Google had for the thing entered by the user. If the thing was not in the list of things google returned, I wanted the user to see a no clue screen.     
      
I attempted to program all of this is in the On Activity Result. This did not work. After re writing it dozens of different ways, I realized the issue. I had really obscured my on Activity Result with many SetContentViews, if checks to see what screen the user was, on then code for each of those views. I realized it didn't make sense to do this all in the on Activity Result and that I really just wanted a function to be called at the end of on activity result where the program flow would stay until the process was restarted. I coded a MainProgramFlow function and that very explicitly and minimally Set Content Views and provided the code for the buttons and functionality in each layout. This worked and the flow was as expected.     
      
Another issue I faced in this assignment was the usefulness of the Google Vision API. I had several issues. First, sometimes google API took so long to respond that the app would think it was not working. After reopening the app and continuing to wait, the image would appear. This was only an issue on phones with higher quality cameras and lower quality internet service. However, it was under these conditions with a higher quality camera that the Vision API most accurately guessed the images. Upon using a lower quality camera phone, the accuracy dramatically decreased, (though time performance certainly increased). When using a lower quality phone, Vision API often returned simply the main color it detected in the image. While this is not technically incorrect, it is definitely a weak point. I would like to explore ways in which to achieve better results from Google API while not sacrificing user experience with slower speeds.    
